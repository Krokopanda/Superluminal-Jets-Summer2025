#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue May 27 14:39:30 2025

@author: mseifer1

Code-only version of https://github.com/pymc-devs/pymc-examples/blob/main/examples/howto/blackbox_external_likelihood_numpy.ipynb
"""

import arviz as az
import matplotlib.pyplot as plt
import numpy as np
import pymc as pm
import pytensor
import pytensor.tensor as pt

from pytensor.graph import Apply, Op
from scipy.optimize import approx_fprime

from simulationImport import importCSV

#put the file path here for the data that is being used
dataSource = '/Users/ivanbijamov/Library/Mobile Documents/com~apple~CloudDocs/SuperluminalJets/isotropic_sims/10000/data_3957522615761_xx_0.8_yy_0.8_zz_0.8.csv'


print(f"Running on PyMC v{pm.__version__}")

# %config InlineBackend.figure_format = 'retina'
az.style.use("arviz-darkgrid")

def my_model(c, x):
    return 1
#overall, x doesnt do anything, so its presence can be ignored for now






def my_loglike(c, sigma, x, data):
    #c is the inverse speed of light, sigma is irrelevant, x is irrelevant, and data is the inverse observed tangent velocity
    #log liklihood function
    if c<=1:
        arg = np.maximum(data**2 + c**2 - 1, 1e-12)  # Keep inside sqrt positive
        #arg = data**2 + c**2 - 1
        numerator = (c**2)*np.sqrt(arg) + (data**2 - c**2)*np.arctan(np.sqrt(arg))
        denominator = (data**2 + c**2)**2
        return np.log(numerator / denominator)
    #revisit function
    elif c>1:
        function = ((2*c*data*(c-1))/(data**2+c**2)**2)\
            +((data*c+(data**2-c**2)*np.arctan(data/c))/(data**2+c**2)**2)
        return np.log(function)
    

# define a pytensor Op for our likelihood function

class LogLike(Op):
    def make_node(self, c, sigma, x, data) -> Apply:
        # Convert inputs to tensor variables
        c = pt.as_tensor(c)
        sigma = pt.as_tensor(sigma)
        x = pt.as_tensor(x)
        data = pt.as_tensor(data)

        inputs = [c, sigma, x, data]
        # Define output type, in our case a vector of likelihoods
        # with the same dimensions and same data type as data
        # If data must always be a vector, we could have hard-coded
        # outputs = [pt.vector()]
        outputs = [data.type()]

        # Apply is an object that combines inputs, outputs and an Op (self)
        return Apply(self, inputs, outputs)

    def perform(self, node: Apply, inputs: list[np.ndarray], outputs: list[list[None]]) -> None:
        # This is the method that compute numerical output
        # given numerical inputs. Everything here is numpy arrays
        c, sigma, x, data = inputs  # this will contain my variables

        # call our numpy log-likelihood function
        loglike_eval = my_loglike(c, sigma, x, data)

        # Save the result in the outputs list provided by PyTensor
        # There is one list per output, each containing another list
        # pre-populated with a `None` where the result should be saved.
        outputs[0][0] = np.asarray(loglike_eval)
        
# set up our data
N = 10  # number of data points
sigma = 1.0  # standard deviation of noise
#Revisit
ctrue = 1.0  # true y-intercept
x=0
truemodel = my_model(ctrue, x)

#Data is generated by mathematica simulation 
dataAll = importCSV(dataSource)
data = [sublist[3] for sublist in dataAll]

# create our Op
loglike_op = LogLike()

def custom_dist_loglike(data, c, sigma, x):
    # data, or observed is always passed as the first input of CustomDist
    return loglike_op(c, sigma, x, data)

test_out = loglike_op(ctrue, sigma, x, data)


# use PyMC to sampler from log-likelihood
with pm.Model() as no_grad_model:

    c = pm.TruncatedNormal("c", lower=0,sigma=0.5)
    #c = pm.Uniform("c",lower=0,upper=10,initval=1)
    # use a CustomDist with a custom logp function
    likelihood = pm.CustomDist(
        "likelihood", c, sigma, x, observed=data, logp=custom_dist_loglike
    )
    
ip = no_grad_model.initial_point()

with no_grad_model:
    # Use custom number of draws to replace the HMC based defaults
    idata_no_grad = pm.sample(10000, tune=1000)

# plot the traces
az.plot_trace(idata_no_grad, lines=[("c", {}, 0)]);
summary = pm.stats.summary(idata_no_grad)
print(summary)